{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center><font color=navy>Revolutionizing Economics:</font></center>\n",
    "### <center><font color=navy>Exploring the Impact of Big Data, Deep Learning, and Quantum Computing</font></center>\n",
    "##### <center><font color=red>Physics of Quantum Information Technology (PQIT) Lab in China - May 23, 2023</font></center>\n",
    "#### <center>Ali Habibnia</center>\n",
    "\n",
    "    \n",
    "<center> Assistant Professor, Department of Economics, </center>\n",
    "<center> and Division of Computational Modeling & Data Analytics at Virginia Tech</center>\n",
    "<center> habibnia@vt.edu </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<img src=\"images/tech3.png\" alt=\"Drawing\" width=\"350\"/>\n",
    "\n",
    "#### Today, we find ourselves at the meeting point of three technologies.\n",
    "\n",
    "* The integration of Big Data, AI, and HPC leads to more sophisticated analytics, allowing us to solve complex problems and gain deeper insights. They enable real-time processing and decision-making in various sectors such as finance, healthcare, and transportation.\n",
    "\n",
    "* Big data provides the raw information that allows AI models to make informed decisions.\n",
    "\n",
    "* HPC provides the computational horsepower needed to handle the processing demands of big data and AI.\n",
    "\n",
    "* AI algorithms can process and analyze vast amounts of data much more quickly and accurately than traditional methods. \n",
    "\n",
    "* A resurgence in the field of artificial neural networks --> Deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Building accurate models is a complex and challenging task. \n",
    "\n",
    "<img src=\"images/model.PNG\" alt=\"Drawing\" width=\"850\"/>\n",
    "\n",
    "Economic forecasting and causal inference are two distinct but interrelated concepts in economics:\n",
    "\n",
    "1. **Economic Forecasting:** This is the process of making predictions about future economic conditions based on current and historical data. Forecasting typically involves using statistical models to extrapolate trends into the future. For example, an economist might use data on employment, inflation, and consumer spending to forecast GDP growth for the next quarter. The primary goal of forecasting is prediction, and these models may not provide a deep understanding of the underlying causal mechanisms driving the trends.\n",
    "\n",
    "2. **Causal Inference:** This is the process of determining whether one variable causes changes in another. Causal inference is more concerned with understanding the underlying mechanisms and relationships in the data. For instance, an economist might be interested in whether and how much increasing minimum wage causes unemployment to rise. Causal inference often involves experimental or quasi-experimental methods, such as randomized controlled trials or instrumental variable regression, which allow economists to isolate the effect of one variable on another. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Econophysics: \n",
    "\n",
    "\n",
    "Many mathematical and computational techniques developed and used in physics, such as differential equations, linear algebra, and optimization, are also widely used in economics and finance. This interdisciplinary field is often called \"econophysics\". \n",
    "\n",
    "1. **Statistical Mechanics:** Economic systems often consist of a large number of interacting agents (consumers, firms, investors, etc.). Statistical mechanics can be used to understand the aggregate behavior of these systems. In particular, methods from statistical mechanics can be used to model income and wealth distributions, price fluctuations, and other phenomena.\n",
    "\n",
    "    - Example: The distribution of wealth has been modeled using methods from statistical mechanics, yielding results similar to Pareto's law.\n",
    "   - Reference: Yakovenko, V. M., & Rosser Jr, J. B. (2009). Colloquium: Statistical mechanics of money, wealth, and income. Reviews of modern physics, 81(4), 1703.\n",
    "   \n",
    "\n",
    "2. **Stochastic Calculus:** In finance, the behavior of asset prices is often modeled as a stochastic (random) process. Methods from stochastic calculus, a branch of mathematics that was originally developed for studying Brownian motion in physics, are frequently used for this purpose. The Black-Scholes-Merton model for option pricing is a well-known application of stochastic calculus.\n",
    "\n",
    "   - Example: The Black-Scholes-Merton model for option pricing is a classic example of the application of stochastic calculus in finance.\n",
    "      - Reference: Black, F., & Scholes, M. (1973). The Pricing of Options and Corporate Liabilities. Journal of Political Economy, 81(3), 637-654.\n",
    "   \n",
    "\n",
    "3. **Complex Systems and Networks:** Economic and financial systems can be seen as complex networks of interacting agents. Concepts and techniques from the physics of complex systems and networks, such as percolation theory and graph theory, can be used to understand the structure and behavior of these systems.\n",
    "\n",
    "   - Example: The global financial network has been modeled as a complex system to understand systemic risk and financial crises.\n",
    "       - Reference: Haldane, A. G., & May, R. M. (2011). Systemic risk in banking ecosystems. Nature, 469(7330), 351-355.\n",
    "       \n",
    "\n",
    "4. **Nonlinear Dynamics and Chaos Theory:** Many economic models involve nonlinear relationships and feedbacks, which can lead to complex dynamics and even chaotic behavior. Techniques from the study of nonlinear dynamical systems and chaos theory in physics can be used to analyze these models. This can be particularly useful for understanding business cycles, financial crises, and other phenomena.\n",
    "\n",
    "    - Example: Nonlinear dynamic models have been used to understand business cycles and financial market volatility.\n",
    "       - Reference: Scheinkman, J. A., & LeBaron, B. (1989). Non-linear dynamics and stock returns. The Journal of Business, 62(3), 311-337.\n",
    "       \n",
    "\n",
    "5. **Thermodynamics and Information Theory:** Concepts from thermodynamics and information theory, such as entropy and the principle of maximum entropy, can be used in economics and finance. For instance, they can be used to derive demand functions in economics, or to measure the amount of \"noise\" or uncertainty in financial markets.\n",
    "\n",
    "   - Example: The principle of maximum entropy has been used to derive demand functions in economics.\n",
    "       - Reference: Golan, A., Judge, G., & Miller, D. (1996). Maximum entropy econometrics: robust estimation with limited data. John Wiley & Sons.\n",
    "\n",
    "\n",
    "6. **Agent-Based Modeling:** While not exclusively a physics method, agent-based modeling, which often uses methods from statistical mechanics and complex systems, has become increasingly popular in economics and finance. These models simulate the actions and interactions of individual agents to understand the emergent behavior of the whole system.\n",
    "\n",
    "   - Example: The Sugarscape model by Epstein and Axtell is an early example of an agent-based model in economics.\n",
    "       - Reference: Epstein, J. M., & Axtell, R. (1996). Growing artificial societies: social science from the bottom up. Brookings Institution Press.\n",
    "\n",
    "\n",
    "7. **Quantum Mechanics:** Although still a niche area, some researchers have started to apply methods from quantum mechanics to economics and finance, particularly to understand decision making under uncertainty and the formation of financial markets.\n",
    "\n",
    "   - Some of the existing quantum speedups that have been identified for algorithms used to solve and estimate economic models: function approximation, linear systems analysis, Monte Carlo simulation, matrix inversion, principal component analysis, linear regression, interpolation, numerical differentiation, and true random number generation.\n",
    "   - Example: Quantum decision theory has been used to model decision making under uncertainty.\n",
    "       - Reference: Busemeyer, J. R., & Bruza, P. D. (2012). Quantum models of cognition and decision. Cambridge University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Big Data?\n",
    "\n",
    "### It depends on who you ask. From an econometric perspective, it could mean:\n",
    "\n",
    "* #### Wild data (unstructured, constract with Census surveys, or twitter)\n",
    "\n",
    "* #### Wide data (a.k.a Larg-P data because p>>N)\n",
    "\n",
    "* #### Long data (a.k.a Large-N data because N is very large and may not even fit onto a single hard drive)\n",
    "\n",
    "* #### Complex model (a.k.a Large-Theta because model/algorithm has many parameters)\n",
    "\n",
    "<img src=\"images/mlp.PNG\" alt=\"Drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pillars of Big Data\n",
    "\n",
    "* #### Foundation of basic calculus, linear algebra, probability analysis, and neumerical optimization)\n",
    "\n",
    "* #### Programming (for automation of data collection, manipulation, cleaning, visualization, and modeling)\n",
    "\n",
    "* #### Visualization & exploration\n",
    "\n",
    "* #### Machine learning (to capture nonlinearity and non normality in data, to compress data, and prediction)\n",
    "\n",
    "* #### Causal inference (to be able to make policy prescription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION: FROM SURVEYS TO SATELLITES AND SENSORS\n",
    "\n",
    "* #### Traditionally, economists have relied on surveys and national account estimates to assess the impact of policy interventions. Conducting surveys is time-intensive, costly, and prone to error. In many countries, notably in the poorest countries and fragile states survey data is simply unavailable or the quality is in doubt.\n",
    "\n",
    "\n",
    "* #### Big data from satellites, mobile phones, and social media, among other tools, allows researchers to build on, and in some cases, replace traditional methods of acquiring socioeconomic data. \n",
    "\n",
    "* #### Its advantages are frequency and near-real-time data, accuracy and objectiveness. Its disadvantages are the fact that the indicators available are merely proxies for what policymakers are interested in and need for policy design.\n",
    "\n",
    "### Examples\n",
    "\n",
    "* #### Social scientists have started to use nighttime light measures, or luminosity, as proxies for economic activity and population distribution. \n",
    "\n",
    "<img src=\"images/satt.jpg\" alt=\"Drawing\" width=\"500\"/>\n",
    "\n",
    "* #### Mobile phone data can also be used to infer socioeconomic characteristics in a geographically disaggregated way.  Cell phones can provide data on: (i) mobility, (ii) social interactions, and (iii) consumption and expenditure patterns.\n",
    "\n",
    "* #### Digital footprints from social media can also fill gaps in data for policymakers and development practitioners. For example, Google Trends (GT) reports, which provide real-time information on search queries at state and metro levels for several countries, have informed private consumption predictions.\n",
    "\n",
    "<img src=\"images/text.PNG\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data: FROM HYPOTHESIS TESTING TO MACHINE LEARNING\n",
    "\n",
    "* ####  Machine learning (ML) allows researchers to analyze data in novel ways. Computers today can process multiple sets of data in little time and, with the correct classification sets, recognize highly complex patterns among them. \n",
    "\n",
    "* #### Designed to simulate the interactions of biological neurons, “deep learning” uses artificial neural networks to discern features in successive layers of data while iterating on previously recognized trends. \n",
    "\n",
    "\n",
    "### Econometrics vs. Machine Learning\n",
    "\n",
    "#### Goal of econometrics\n",
    "\n",
    "* #### \"the goal of econometrics is to find β hat\" where here we mean β hat to be the causal impact of X on y\n",
    "\n",
    "* #### The primary statistical concern of econometrics is sampling error. In other words, the goal is to quantify the uncertainty around β hat due to randomness in the sampling of the population. \n",
    "\n",
    "* #### The goal is to make counterfactual predictions.\n",
    "\n",
    "        What would happen to Amazon's profits if it changed its website layout?\n",
    "\n",
    "We don't get to observe the world under these alternative policies, so we can't simply find the answers in the data. Knowing the counterfactual requires being able to measure a causal effect. Being able to measure a causal effect requires making assumptions. That's what economics is all about!\n",
    "\n",
    "\n",
    "#### Goal of machine learning\n",
    "\n",
    "* #### In contrast, the goal of machine learning is to come up with the best possible out-of-sample prediction. \n",
    "\n",
    "* #### We refer to this as the primary concern of machine learning being \"y hat\"\n",
    "\n",
    "* #### The goal is to make sure that the best prediction is had by tuning and validating many different kinds of models. This is what cross-validation is all about, and it is what machine learning practitioners obsess about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  $  y = X\\beta + \\varepsilon $   </center>\n",
    "<br>\n",
    "<center>  $\\varepsilon \\sim N(0,\\sigma^2) $   </center>\n",
    "<br>\n",
    "<center>  $  L_{OLS}(\\hat\\beta) = \\sum_{i=1}^n (y_i - x_i' \\hat\\beta)^2 = ||y-X'\\beta||^2$   </center>\n",
    "<br>\n",
    "<center>  $\\hat\\beta_{OLS} = (X′X)^{−1}(X′Y) $   </center>\n",
    "\n",
    "> In statistics, there are two critical characteristics of estimators to be considered: the **bias** and the **variance**. \n",
    "\n",
    "The bias is the difference between the true population parameter and the expected estimator:\n",
    "\n",
    "<br>\n",
    "<center>  $Bias (\\hat\\beta_{OLS}) = E(\\hat\\beta_{OLS}) - \\beta $   </center>\n",
    "\n",
    "It measures the accuracy of the estimates. Variance, on the other hand, measures the spread, or uncertainty, in these estimates. It is given by\n",
    "\n",
    "<br>\n",
    "<center>  $Var (\\hat\\beta_{OLS}) = \\sigma^2(X′X)^{−1} $   </center>\n",
    "\n",
    "where the unknown error variance σ2 can be estimated from the residuals as\n",
    "\n",
    "<center>  $\\sigma^2 = \\frac{\\varepsilon' \\varepsilon}{n-p} $   </center>\n",
    "\n",
    "> #### Both the bias and the variance are desired to be low, as large values result in poor predictions from the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Absolute Shrinkage and Selection Operator (LASSO, L1 Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero. Variables with a regression coefficient equal to zero after the shrinkage process are excluded from the model. Variables with non-zero regression coefficients variables are most strongly associated with the response variable. Explanatory variables can be either quantitative, categorical or both.\n",
    "\n",
    "#### Model Specification\n",
    "\n",
    "Lasso regression takes a different approach. Instead of adding the sum of squared $\\beta$ coefficients to the loss function, it adds the sum of the absolute values of the $\\beta$ coefficients:\n",
    "\n",
    "<br>\n",
    "<center>  $L_{LASSO}(\\hat\\beta) = \\sum_{I=1}^n(y_i-x_i’\\hat\\beta)^2 + \\lambda \\sum_{j=1}^p|\\hat\\beta| = ||y-X\\hat\\beta||^2 + \\lambda||\\hat\\beta||$   </center>\n",
    "\n",
    "This is equivalent to saying minimizing the loss/cost function under the condition as below\n",
    "\n",
    "<br>\n",
    "<center>  for some $~ C > 0, ~~ \\sum_{j=1}^p|\\hat\\beta|\\leq C$ </center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction & Dimensionality Reduction with Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are one of the unsupervised deep learning models. The aim of an autoencoder is dimensionality reduction and feature discovery. An autoencoder is trained to predict its own input, but to prevent the model from learning the identity mapping, some constraints are applied to the hidden units. \n",
    "\n",
    "“Autoencoding” is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term “autoencoder” is used, the compression and decompression functions are implemented with neural networks.\n",
    "\n",
    "For building an autoencoder, three things are needed: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a “loss” function).\n",
    "\n",
    "The simplest form of an autoencoder is a feedforward neural network where the input $x$ is fed to the hidden layer and the output of hidden layer is then fed to reconstruct the original inputs. A simple autoencoder is shown below:\n",
    "\n",
    "<center><img src=\"images/autoencoder.png\" width=\"650\" alt=\"A simple Autoencoder.\"></center>\n",
    "<center><img src=\"images/autoencoder2.png\" width=\"850\" alt=\"A simple Autoencoder.\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders map the data they are fed to a lower dimensional space by combining the data’s most important features. It encodes the original data into a more compact representation. It also decides how the data is combined, hence the auto in Autoencoder. We refer to these encoded features as latent variables.\n",
    "\n",
    "There are a few reasons doing this may be useful:\n",
    "\n",
    "1. Dimensionality reduction can decrease training time.\n",
    "1. Using latent feature representations may enhance model performance.\n",
    "\n",
    "\n",
    "Like the Autoencoder model, Principal Components Analysis (PCA) is also widely used as a dimensionality reduction technique. However, PCA maps the input in a different way than an Autoencoder.\n",
    "\n",
    "Both PCA and Autoencoder can be thought of as a lossy data compression algorithm. The quality of the data is not perfectly retained -- some of the original data get lost. Notice that it is different with feature selection. Feature selection algorithms discard some features of the data and retain salient features. The features they retain are typically chosen for statistical reasons, such as the correlation between the attribute and the target label. Autoencoder, as a feature extraction algorithm, projects data into a new space.\n",
    "\n",
    "Let's recap how PCA worked first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA works by projecting input data onto the eigenvectors of the data’s covariance matrix. The covariance matrix quantifies the variance of the data and how much each variable varies with respect to one another. Eigenvectors are simply vectors that retain their span through a linear transformation, that is, they point in the same direction before and after the transformation. The covariance matrix transforms the original basis vectors to be oriented in the direction of the covariance between each variable. In simpler terms, the eigenvector allows us to re-frame the orientation of the original data to view it at a different angle without actually transforming the data. We are essentially extracting the component of each variable that leads to the most variance when we project the data onto these vectors. We can then select the dominant axes using the eigenvalues of the covariance matrix because they reflect the magnitude of the variance in the direction of their corresponding eigenvector.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1ifTzuIC47I1DTP88cwUvtRQtG-_5OQIy\" width=\"450\" alt=\"A simple Autoencoder.\"></center>\n",
    "\n",
    "These projections result in a new space, where each basis vector encapsulates the most variance (i.e. the projections onto the eigenvector with the largest eigenvalue have the most variance, the ones on the second eigenvector have the second most variance, etc.). These new basis vectors are referred to as the principal components. We want principal components to be oriented in the direction of maximum variance because greater variance in attribute values can lead to better forecasting abilities. For example, say you’re trying to predict the price of a car given two attributes: color and brand. Suppose all the cars have the same color, but there are many brands among them. Guessing a car’s price based on its color — a feature with zero variance — would be pretty much impossible in this example. However, if we considered a feature with more variance, the brand, we will be able to come up with better price estimates because Audis and Ferraris tend to be priced higher than Hondas and Toyotas. The principal components resulting from PCA are linear combinations of the input variables . The linear nature of these principal components also allow us to interpret the transformed data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
